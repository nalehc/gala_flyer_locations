{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports a library 'pandas', names it as 'pd'\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "from bisect import bisect\n",
    "import pickle\n",
    "import csv\n",
    "filename = '/home/williamcottrell72/github/sf18_ds11/class_lectures/week01-benson/02-git_viz/turnstiles.pkl'\n",
    "from IPython.display import Image\n",
    "\n",
    "# enables inline plots, without it plots don't show up in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# fix_time() is for formatting the week_nums appropriately.  \n",
    "\n",
    "def fix_time(num):\n",
    "    if len(str(num)) == 2:\n",
    "        return str(num)\n",
    "    else:\n",
    "        return '0'+str(num)\n",
    "    \n",
    "# User inputs a month number (6=June, etc) and the number of years back.  The get_weeks_nums()\n",
    "#function will return a list of week numbers formatted in such a way that they can be input\n",
    "#into the mta url for web-scraping.\n",
    "\n",
    "def get_week_nums(month,yrs_back):\n",
    "    week_list=[]\n",
    "    ref_date=datetime.date(2018,6,30)\n",
    "    weeks_back=yrs_back*52\n",
    "    for i in range(weeks_back):\n",
    "        week_shift=datetime.timedelta(-7*i)\n",
    "        new=ref_date+week_shift\n",
    "        yr=str(new.year)[-2:]\n",
    "        mt=fix_time(new.month)\n",
    "        day=fix_time(new.day)\n",
    "        string=yr+mt+day\n",
    "        if int(mt)==month:\n",
    "            week_list.append(int(string))\n",
    "    return week_list\n",
    "\n",
    "# scrape_pkl scrapes the data, returns a dataframe, and saves the result in a file.\n",
    "\n",
    "def scrape_pkl(week_nums,filename):\n",
    "    try:\n",
    "        with open(filename,'rb') as pklfile:\n",
    "            df = pickle.load(pklfile)\n",
    "    except:\n",
    "\n",
    "        df = scrape(week_nums)\n",
    "\n",
    "        with open(filename,'wb') as pklfile:\n",
    "            df = pickle.dump(df, pklfile)\n",
    "    return df\n",
    "\n",
    "#\n",
    "#def fn(row):\n",
    "#    return (row['datetime'].second==0)&(row['datetime'].minute==0)\n",
    "\n",
    "\"\"\"\n",
    "Cleaning proceeds as follows:  \n",
    "1) Strip whitespace in column names \n",
    "2) add datetime column,\n",
    "3) drop New Jersey (PTH), drop duplicates, \n",
    "4) group by station and datetime, take the sum over exits.\n",
    "\"\"\"\n",
    "\n",
    "def clean_df(df):\n",
    "    cols={x:x.strip() for x in df.columns}\n",
    "    df_small=df.rename(columns=cols)\n",
    "    df_small['datetime']=pd.to_datetime(df_small['DATE']+' '+df_small['TIME'],infer_datetime_format=True)\n",
    "    df_small_clean=df_small[df_small.DIVISION!='PTH'].drop_duplicates(subset=['C/A','UNIT','SCP','STATION','LINENAME','datetime'])\n",
    "    df_small_clean2=df_small_clean.groupby(['STATION','datetime'],as_index=False)[['EXITS']].sum()\n",
    "    return df_small_clean2\n",
    "\n",
    "\"\"\"\n",
    "station_activity(df,station) returns a dataframe for the specified station.  Columns are \n",
    "sorted in order of datetime.\n",
    "\"\"\"\n",
    "\n",
    "def station_activity(df,station):\n",
    "    df_station=df[df['STATION']==station]\n",
    "    df_sort=df_station.sort_values(by=['datetime'])\n",
    "    return df_sort\n",
    "\n",
    "\"\"\"\n",
    "Next we make a dictionary whose keys are stations and values are a list of number of people\n",
    "exiting per four hour period.\n",
    "\"\"\"\n",
    "\n",
    "def construct_dct(df):\n",
    "    stations=df['STATION'].unique()\n",
    "    station_diffs_dct={}\n",
    "    for st in stations:\n",
    "        sa=station_activity(df,st)\n",
    "        sa['diffs']=pd.DataFrame(sa['EXITS'].diff())\n",
    "        st_diffs=sa.drop(['EXITS'],axis=1).dropna()\n",
    "        st_diffs_clean=st_diffs[np.abs(st_diffs.diffs)<30*10**3]    \n",
    "        st_diffs_clean['weekday']=st_diffs_clean['datetime'].dt.weekday\n",
    "        st_diffs_clean['hour']=st_diffs_clean['datetime'].dt.hour\n",
    "        st_diffs_clean2=st_diffs_clean.groupby(['weekday','hour'])['diffs'].mean()\n",
    "        station_diffs_dct[st]=st_diffs_clean2\n",
    "    return station_diffs_dct\n",
    "\n",
    "\"\"\"\n",
    "Since different stations are on different schedules we need a helper function, \n",
    "find_key(), which allows us to find the appropriate time-interval to associate \n",
    "with a user-entered hour.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def find_key(num,hours_list):\n",
    "    sort=sorted(hours_list)\n",
    "    pos=bisect(sort,num)\n",
    "    if pos < len(sort):\n",
    "        return sort[pos]\n",
    "    else:\n",
    "        return sort[0]\n",
    "    \n",
    "\"\"\"\n",
    "Now, for each day of the week and hour, we construct a list of stations in descending \n",
    "order of the number of people leaving.\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "def activity_by_time(day,hour,dct):\n",
    "    stations=list(set(dct.keys()))\n",
    "    exits=[]\n",
    "    for st in stations:\n",
    "        try:\n",
    "            \"\"\"Below we are assuming that the hour list appearing is consistent for\n",
    "            a given station. (I take the first element of the list as being represent-\n",
    "            ative.) Should check this\"\"\"\n",
    "            hours_st=dct[st][0].keys().values\n",
    "            sh=sorted(hours_st)\n",
    "            h_key=find_key(hour,sh)\n",
    "            leaving=dct[st][day][h_key]/4\n",
    "            \"\"\"\n",
    "            Below I apply np.abs to account for the possibility that the turnstile was\n",
    "            reversed for some period of time.\n",
    "            \"\"\"             \n",
    "            exits.append([st,np.abs(leaving)])\n",
    "        except(KeyError,IndexError,AttributeError):\n",
    "            pass\n",
    "    sort_exits=sorted(exits,key=lambda x: x[1])[::-1]    \n",
    "    return sort_exits\n",
    "\n",
    "\n",
    "def main(month,day,hour,yrs_back=3):\n",
    "    week_nums=get_week_nums(month,yrs_back)\n",
    "    df=scrape_pkl(week_nums,filename)\n",
    "    df_c=clean_df(df)\n",
    "    dct=construct_dct(df_c)\n",
    "    \n",
    "    return (activity_by_time(day,hour,dct),dct,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we run 'main(6,4,5)' which gives us the activity for month 6 = June, day 4 = Friday, hour 5 = 5am.  The 'yrs_back' variable is set to 3 by default.  We also return the full dataframe from the last 3 years (df) and the cleaned dictionary with station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/williamcottrell72/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/williamcottrell72/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "activity,dct,df = main(6,4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have constructed a function which shows the top stations for a given hour of the week.  We might also want to know the stations with lots of people exiting on average.  We can use the dct and df output from the previous function and take averages. We will focus on afternoon averages for specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['TIMES SQ-42 ST', 3151.29935515873],\n",
       "       ['14 ST-UNION SQ', 2986.3298611111113],\n",
       "       ['34 ST-HERALD SQ', 2375.5850694444443],\n",
       "       ['FLUSHING-MAIN', 2036.685267857143],\n",
       "       ['ATL AV-BARCLAY', 1687.7100694444446],\n",
       "       ['JKSN HT-ROOSVLT', 1686.421875],\n",
       "       ['59 ST COLUMBUS', 1648.171875],\n",
       "       ['BEDFORD AV', 1446.298859126984],\n",
       "       ['59 ST', 1341.6450892857142],\n",
       "       ['W 4 ST-WASH SQ', 1333.3482142857142],\n",
       "       ['50 ST', 1105.7150297619048],\n",
       "       ['145 ST', 1080.6830357142858],\n",
       "       ['JAMAICA CENTER', 1060.3177083333333],\n",
       "       ['7 AV', 862.4196428571429],\n",
       "       [\"B'WAY-LAFAYETTE\", 745.077380952381],\n",
       "       ['CHAMBERS ST', 720.2983630952381],\n",
       "       ['KEW GARDENS', 716.6532738095239],\n",
       "       ['8 AV', 685.797619047619],\n",
       "       ['42 ST-BRYANT PK', 634.0885416666666],\n",
       "       ['JUNCTION BLVD', 590.2388392857143]], dtype=object)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_big_df(dct):\n",
    "    df_cum=pd.DataFrame(activity_by_time(0,12,dct)[:20])\n",
    "    for i in range(1,12):\n",
    "        for day in range(1,7):\n",
    "            df2=pd.DataFrame(activity_by_time(day,12+i,dct)[:20])\n",
    "            df_cum=pd.merge(df_cum,df2,on=0,how='outer')\n",
    "    return df_cum\n",
    "\n",
    "def clean_df_cum(dct):\n",
    "    df_cum=make_big_df(dct)\n",
    "    df_fill=df_cum.fillna(0)\n",
    "    df_fill.insert(1,'sums',df_fill.sum(axis=1)/84)\n",
    "    fin_result=df_fill[[0,'sums']].sort_values(['sums'],ascending=False).head(20)\n",
    "    return fin_result.values\n",
    "\n",
    "clean_df_cum(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['TIMES SQ-42 ST', 4581.5625],\n",
       " ['34 ST-HERALD SQ', 4291.375],\n",
       " ['14 ST-UNION SQ', 3825.25],\n",
       " ['FLUSHING-MAIN', 2370.4375],\n",
       " ['59 ST COLUMBUS', 2202.1875],\n",
       " ['ATL AV-BARCLAY', 2042.875],\n",
       " ['W 4 ST-WASH SQ', 2026.4375],\n",
       " ['BEDFORD AV', 1990.0],\n",
       " ['JKSN HT-ROOSVLT', 1879.125],\n",
       " ['59 ST', 1597.9375]]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_by_time(5,18,dct)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
