{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic:**        Project Benson   \n",
    "**Subject:**      Explore MTA turnstile data  \n",
    "**Date:**         07/03/2018  \n",
    "**Name:**         Auste Mastaviciute  \n",
    "**Worked with:**  Billy, Chelan, Alan, Joyce, Xu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import datetime\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various options in pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Data from MTA website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(num):\n",
    "    if len(str(num)) == 2:\n",
    "        return str(num)\n",
    "    else:\n",
    "        return '0'+str(num)\n",
    "\n",
    "def get_week_nums(month,yrs_back):\n",
    "    week_list = []\n",
    "    ref_date = datetime.date(2018,6,30)\n",
    "    weeks_back = yrs_back * 52\n",
    "    for i in range(weeks_back):\n",
    "        week_shift = datetime.timedelta(-7 * i)\n",
    "        new = ref_date + week_shift\n",
    "        yr = str(new.year)[-2:]\n",
    "        mt = fix_time(new.month)\n",
    "        day = fix_time(new.day)\n",
    "        string = yr + mt + day\n",
    "        if int(mt) == month:\n",
    "            week_list.append(int(string))\n",
    "    return week_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-44dc8b1afc60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mweek_nums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_week_nums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mturnstiles_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweek_nums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mturnstiles_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-44dc8b1afc60>\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(week_nums)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mweek_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweek_nums\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfile_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweek_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[0;32m--> 433\u001b[0;31m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gzip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readall_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_readall_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_chunk_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_get_chunk_left\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# toss the CRLF at the end of the chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                 \u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_next_chunk_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_next_chunk_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# Read the next chunk size from the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def scrape(week_nums):\n",
    "    \"\"\"Gets the data from MTA website and loads it into a pandas DataFrame\"\"\"\n",
    "    url = 'http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt'\n",
    "    dfs = []\n",
    "    for week_num in week_nums:\n",
    "        file_url = url.format(week_num)\n",
    "        dfs.append(pd.read_csv(file_url))\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "week_nums = get_week_nums(6, 3)\n",
    "turnstiles_df = scrape(week_nums)\n",
    "turnstiles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Formating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "turnstiles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trim the names of the columns from all the unneccesary white space\n",
    "turnstiles_df.columns = turnstiles_df.columns.str.strip()\n",
    "turnstiles_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the DATE and TIME columns and format as DateTime rather than string\n",
    "turnstiles_df['DATETIME'] = pd.to_datetime(turnstiles_df['DATE'] +' '+ turnstiles_df['TIME'], infer_datetime_format=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add WEEKDAY column\n",
    "turnstiles_df['WEEKDAY'] = turnstiles_df['DATETIME'].dt.weekday_name\n",
    "#turnstiles_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates\n",
    "duplicate_check = turnstiles_df.groupby(['C/A', 'UNIT', 'SCP', 'STATION', 'WEEKDAY','DATETIME']).EXITS.count().reset_index().sort_values(by='EXITS', ascending = False)\n",
    "#print(duplicate_check.head(5),'\\n')\n",
    "\n",
    "duplicate_count = duplicate_check[duplicate_check['EXITS'] > 1]['STATION'].count()\n",
    "#print(duplicate_count)\n",
    "print('The dataset had', str(duplicate_count), 'duplicated values for station & datetime pairs, which we will remove in the next step')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "turnstiles_df.sort_values(['C/A', 'UNIT', 'SCP', 'STATION', 'WEEKDAY','DATETIME'], inplace = True, ascending = False)\n",
    "turnstiles_df.drop_duplicates(subset = ['C/A', 'UNIT', 'SCP', 'STATION', 'WEEKDAY','DATETIME'], inplace = True)\n",
    "\n",
    "#Check if duplicates are gone\n",
    "duplicate_double_check = turnstiles_df.groupby(['C/A', 'UNIT', 'SCP', 'STATION', 'WEEKDAY','DATETIME']).EXITS.count().reset_index().sort_values(by='EXITS', ascending = False)\n",
    "final_duplicates = duplicate_double_check[duplicate_double_check['EXITS'] > 1]['STATION'].count()\n",
    "remaining = 'All' if final_duplicates == 0 else str(final_duplicates)\n",
    "\n",
    "print(str(remaining), 'duplicates have been removed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the different time intervals**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand the distribution among different times\n",
    "time_dist = turnstiles_df.groupby([turnstiles_df['DATETIME'].dt.time])['EXITS'].count().sort_values(ascending=False)\n",
    "time_dist.plot(figsize = (10, 5), xticks = ['00:00:00', '04:00:00', '08:00:00', '12:00:00', '16:00:00', '20:00:00'], color='g')\n",
    "plt.title('Distribution of values across 4 hour intervals')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the stations with non-zero hour records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_exact_hour_df = turnstiles_df[(turnstiles_df['DATETIME'].dt.minute != 0) | (turnstiles_df['DATETIME'].dt.second != 0)]\n",
    "\n",
    "print(non_exact_hour_df.groupby(['DIVISION'])['EXITS'].count().sort_values(ascending = False))\n",
    "print('\\nMost (in this example - 74%) of the records are coming from PTH division, which is in New Jersey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove New Jersey (PTH) stations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of stations before dropping PTH division was', str(len(turnstiles_df['STATION'].unique())))\n",
    "\n",
    "turnstile_no_PTH = turnstiles_df[turnstiles_df['DIVISION'] != 'PTH']\n",
    "#turnstile_no_PTH[turnstile_no_PTH['DIVISION'] == 'PTH']['STATION'].count()\n",
    "\n",
    "total_records = turnstiles_df['DATETIME'].count()\n",
    "total_no_PTH_records = turnstile_no_PTH['DATETIME'].count()\n",
    "\n",
    "print('Number of stations after dropping PTH division was', str(len(turnstile_no_PTH['STATION'].unique())))\n",
    "print('Number stations dropped as New Jersey stations is', str(len(turnstiles_df['STATION'].unique()) - len(turnstile_no_PTH['STATION'].unique()))+'.')\n",
    "\n",
    "print(str(round(((total_records - total_no_PTH_records) / total_no_PTH_records) * 100, 0))+ '% of all records have been excluded as New Jersey data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore remaining non-exact hours records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantify how many records there are with non-zero minute:second pairs\n",
    "exact_hour = turnstile_no_PTH['STATION'][(turnstile_no_PTH['DATETIME'].dt.minute == 0) & (turnstile_no_PTH['DATETIME'].dt.second == 0)].count()\n",
    "non_exact_hour = turnstile_no_PTH['STATION'][(turnstile_no_PTH['DATETIME'].dt.minute != 0) | (turnstile_no_PTH['DATETIME'].dt.second != 0)].count()\n",
    "\n",
    "print('Number of stations with minutes equal to zero:', exact_hour)\n",
    "print('Number of stations with minutes not equal to zero:', non_exact_hour)\n",
    "print(str(round((non_exact_hour / (exact_hour + non_exact_hour)) * 100, 0))+ '% of records were not recorded at exact hour.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of stations before cleaning non-exact hour records is', str(len(turnstile_no_PTH['STATION'].unique()))+'.')\n",
    "\n",
    "#Remove timestamps with non-zero minutes and seconds\n",
    "turnstiles_clean = turnstile_no_PTH[(turnstile_no_PTH['DATETIME'].dt.minute == 0) & (turnstile_no_PTH['DATETIME'].dt.second == 0)]\n",
    "print('Number of stations after cleaning non-exact hour records is', str(len(turnstiles_clean['STATION'].unique()))+'.')\n",
    "\n",
    "excluded = turnstile_no_PTH[(turnstile_no_PTH['DATETIME'].dt.minute != 0) | (turnstile_no_PTH['DATETIME'].dt.second != 0)]\n",
    "incl_stations = turnstiles_clean['STATION'].unique()\n",
    "excl_stations = excluded['STATION'].unique()\n",
    "num_stations_excl = len(turnstile_no_PTH['STATION'].unique()) - len(turnstiles_clean['STATION'].unique())\n",
    "\n",
    "def dropped_stations(incl_stations, excl_stations):\n",
    "    \"\"\"Takes the list of included stations and list of excluded stations.\n",
    "    Returns the stations that have been completely droped\"\"\"\n",
    "    dropped_stations = []\n",
    "    for station in excl_stations:\n",
    "        if station not in incl_stations:\n",
    "            dropped_stations.append(station)\n",
    "    return dropped_stations\n",
    "\n",
    "print('The following', str(num_stations_excl), 'stations have been excluded from the dataset:', str(dropped_stations(incl_stations, excl_stations)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understand if the stations to be excluded are significant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_st = turnstile_no_PTH.loc[turnstile_no_PTH['STATION'].isin(['KEW GARDENS', '161/YANKEE STAD', '15 ST-PROSPECT'])]\n",
    "#dropped_st.head()\n",
    "\n",
    "#161/YANKEE STAD station consistently has the records every 4 hours (starting midnight) and 22 minutes\n",
    "#15 ST-PROSPECT and KEW GARDENS stations consistently has the records every 4 hours (starting midnight) and 30 minutes\n",
    "\n",
    "#Should probably leave these stations in, since they are recorded at a legitimate 4h period\n",
    "\n",
    "print('The', str(num_stations_excl), 'stations that were being excluding through non-exact hour cleaning process proved to be valuable and therefore will be included in our analysis.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the data frame by DATETIME column\n",
    "turnstile_no_PTH = turnstile_no_PTH.sort_values(by = ['STATION', 'DATETIME'])\n",
    "#turnstile_no_PTH.head(15)\n",
    "\n",
    "#Add two new columns with Previous Datetime a record was taken and the actual record in PREV_EXITS\n",
    "turnstile_no_PTH['PREV_DATETIME'] = turnstile_no_PTH['DATETIME'].shift()\n",
    "turnstile_no_PTH['PREV_EXITS'] = turnstile_no_PTH['EXITS'].shift()\n",
    "#turnstile_no_PTH.head()\n",
    "\n",
    "#Add EXITS_DIFF column - Transform the Exits column from accummulative to actual per time period\n",
    "turnstile_no_PTH['EXITS_DIFF'] = turnstile_no_PTH['EXITS'].sub(turnstile_no_PTH['PREV_EXITS'])\n",
    "turnstile_no_PTH.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop NaN values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows with NaN values in previous exits column\n",
    "turnstile_no_PTH = turnstile_no_PTH.drop(['EXITS_DIFF'], axis=0).dropna()\n",
    "turnstile_no_PTH.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shorten the DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstile_short = turnstile_no_PTH.groupby(['STATION', 'WEEKDAY','DATETIME'])['EXITS_DIFF'].sum()\n",
    "turnstile_short = turnstile_short.reset_index()\n",
    "turnstile_short.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting all EXITS_DIFF to positive values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstile_short['EXITS_DIFF'] = abs(turnstile_short['EXITS_DIFF'])\n",
    "turnstile_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore the distribution of EXITS_DIFF column\n",
    "turnstile_short['EXITS_DIFF'].plot(title = 'Absolute Number of Exits Distribution', figsize = (10, 5), color = 'c')\n",
    "plt.xlabel('Records per station & Date-Time pair')\n",
    "plt.ylabel('Exits')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove the extreme values and outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstile_less_mln = turnstile_short.copy()\n",
    "#turnstile_less_mln.head()\n",
    "\n",
    "new_exits_mln = turnstile_less_mln.loc[turnstile_less_mln['EXITS_DIFF'] < 1 * 10**6]['EXITS_DIFF']\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.hist(new_exits_mln, bins=np.arange(0, 3 * 10**5, 30), color='m', log = True)\n",
    "plt.title('Exits distribution per 4 hour interval')\n",
    "plt.xlabel('Exits (limit = 300k)')\n",
    "plt.ylabel('Frequency (log)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal y-scale with limit of 3000\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.hist(new_exits_mln, bins=np.arange(0, 3 * 10**3, 30), color='m')\n",
    "plt.title('Exits distribution per 4 hour interval')\n",
    "plt.xlabel('Exits (limit = 3000)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
