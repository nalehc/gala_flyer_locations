{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Tech Companies in New York City\n",
    "\n",
    "### Joyce Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from googleplaces import GooglePlaces, types, lang\n",
    "import googlemaps\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import geopy\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"API_joyce_ignore\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    google_places_api = lines[0].split(\":\")[1].strip()\n",
    "\n",
    "API_KEY = google_places_api\n",
    "google_places = GooglePlaces(API_KEY)\n",
    "gmaps = googlemaps.Client(key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most valuable tech companies in the US\n",
    "\n",
    "Use BeautifulSoup to scrape a BusinessInsider article to get a list of the top 21 most valuable American tech companies. Webscraping functions taken from following website: https://realpython.com/python-web-scraping-practical-introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscraping function to  article to get list of the top most valuable tech companies in America.\n",
    "\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you view the page source of the Business Insider article, you can see that each company name is contained within an H2 tag with the class 'slide-title-text'. Use BeautifulSoup to find all the elements that meet these criteria, then clean up the list to get the names of the companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Qualcomm', 'Western Digital', 'eBay', 'Adobe', 'HPE', '3M', 'HP', 'booking', 'NETFLIX', 'Dell', 'Uber', 'Cisco', 'Intel', 'YouTube', 'Oracle', 'IBM', 'Microsoft', 'Facebook', 'Google', 'Apple', 'Amazon']\n"
     ]
    }
   ],
   "source": [
    "raw_html = simple_get('http://www.businessinsider.com/amazon-google-apple-most-valuable-tech-brands-america-2018-6')\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "BI_companies = soup.find_all(\"h2\", class_=\"slide-title-text\")\n",
    "BI_company_list = [company.text for company in BI_companies]\n",
    "BI_company_list = [company.split(\".\")[1].strip() for company in BI_company_list]\n",
    "print(BI_company_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, define a function that iterates through the list of companies, and for each company query google_places with a text search for its office in New York. This query returns an array of 'Places'. Store these results in a dict, where the key is the name of the company and the value is the array of 'Places'.\n",
    "\n",
    "Next, the verify_places function takes this dictionary and returns a DataFrame. It iterates through the dictionary, and for each company, go through the array of Places and attempt to verify if each 'Place' really is affiliated with the company. A simple way to do this is to check to see if the name of the company is in the 'Place' name, and also if the substring 'company name'.com is located in the 'Place' website. If both of these criteria are met, then this Place is likely to be truly associated with the company, and an entry for this 'Place' is inserted into the dataframe. Once the function is done iterating through the entire dictionary, the populated DataFrame is returned.\n",
    "\n",
    "Finally, the make_geodf function turns the dataframe into a geo-dataframe, so that this can then be written to a geojson file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_company_places_dict(company_list):\n",
    "    company_place_dict = {}\n",
    "    place_query = '{} office in New York City'\n",
    "    for company in company_list:\n",
    "        company_place_dict[company] = google_places.text_search(query=place_query.format(company))\n",
    "    return company_place_dict\n",
    "\n",
    "def verify_places(company_place_dict):\n",
    "    company_df = pd.DataFrame(columns=['company_name', 'latitude', 'longitude'])\n",
    "    \n",
    "    for company in company_place_dict:\n",
    "        for place in company_place_dict[company].places:\n",
    "            score = 0\n",
    "            place.get_details()\n",
    "            if company.lower() in place.name.lower():\n",
    "                score += 1\n",
    "            if (company.lower() + \".com\") in str(place.website).lower():\n",
    "                score += 1\n",
    "            if score == 2:\n",
    "                row = pd.Series({'company_name':company, 'latitude':place.geo_location['lat'], 'longitude':place.geo_location['lng']})\n",
    "                company_df = company_df.append(row, ignore_index=True)\n",
    "                \n",
    "    return company_df\n",
    "\n",
    "def make_geodf(company_df):\n",
    "    company_df['coordinates'] = list(zip(company_df['longitude'], company_df['latitude']))\n",
    "    company_df['coordinates'] = company_df['coordinates'].apply(Point)\n",
    "    company_geodf = gpd.GeoDataFrame(company_df).set_geometry('coordinates')\n",
    "    \n",
    "    company_geodf['latitude'] = pd.to_numeric(company_geodf['latitude'])\n",
    "    company_geodf['longitude'] = pd.to_numeric(company_geodf['longitude'])\n",
    "    company_geodf['company_name'] = company_geodf['company_name'].astype('str')\n",
    "    \n",
    "    return company_geodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_dict = make_company_places_dict(BI_company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_verified_places_df = verify_places(BI_dict)\n",
    "BI_geodf = make_geodf(BI_verified_places_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_geodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BI_geodf.to_file(\"BI_companies.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = gpd.read_file('BI_companies.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = \"community_districts.geojson\"\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(6,8))\n",
    "companies = companies.set_geometry('geometry')\n",
    "bkg = gpd.read_file(background)\n",
    "base = bkg.plot(ax = ax1, color = 'white', edgecolor='black')\n",
    "fig = companies.plot(ax=base, markersize = 50, legend = True, column = 'company_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 100 NYC tech companies\n",
    "\n",
    "Let's scrape a different article, this time an article published by BuiltInNYC listing the top 100 NYC-based tech companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = simple_get('https://www.builtinnyc.com/2017/11/07/nyc-top-100-tech-companies-2017')\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "top_100 = soup.find_all(\"div\", class_=\"company-info-wrapper\")\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "top_100_company_list = []\n",
    "\n",
    "for company in top_100:\n",
    "    s = company.find_all(\"a\", href=re.compile(\"https://www.builtinnyc.com/company/\"))\n",
    "    top_100_company_list.append(str(s))\n",
    "\n",
    "top_100_company_list = list(map(remove_html_tags, top_100_company_list))\n",
    "top_100_company_list = [company.split(\",\")[1].strip() for company in top_100_company_list]\n",
    "top_100_company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_dict = make_company_places_dict(top_100_company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_verified_places_df = verify_places(top_100_dict)\n",
    "top_100_geodf = make_geodf(top_100_verified_places_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_geodf.to_file(\"top_100_tech_nyc.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = gpd.read_file('top_100_tech_nyc.geojson')\n",
    "\n",
    "background = \"community_districts.geojson\"\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(8,8))\n",
    "companies = companies.set_geometry('geometry')\n",
    "bkg = gpd.read_file(background)\n",
    "base = bkg.plot(ax = ax1, color = 'white', edgecolor='black')\n",
    "fig = companies.plot(ax=base, markersize = 50, legend = True, column = 'company_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = simple_get('https://www.builtinnyc.com/2018/01/16/50-nyc-startups-watch-2018')\n",
    "soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "new_50_2018 = soup.find_all(\"div\", class_=\"company-info-wrapper\")\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "company_list = []\n",
    "\n",
    "for company in new_50_2018:\n",
    "    test = company.find_all(\"a\", href=re.compile(\"https://www.builtinnyc.com/company/\"))\n",
    "    company_list.append(str(test))\n",
    "\n",
    "clean_company_list = list(map(remove_html_tags, company_list))\n",
    "clean_company_list = [company.split(\",\")[1].strip() for company in clean_company_list]\n",
    "clean_company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50 = make_company_places_dict(clean_company_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_df = verify_places(top_50)\n",
    "top_50_geodf = make_geodf(top_50_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_geodf.to_file(\"top_50_tech_nyc.geojson\", driver='GeoJSON')\n",
    "companies = gpd.read_file('top_50_tech_nyc.geojson')\n",
    "\n",
    "background = \"community_districts.geojson\"\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(8,8))\n",
    "companies = companies.set_geometry('geometry')\n",
    "bkg = gpd.read_file(background)\n",
    "base = bkg.plot(ax = ax1, color = 'white', edgecolor='black')\n",
    "fig = companies.plot(ax=base, markersize = 50, legend = True, column = 'company_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
